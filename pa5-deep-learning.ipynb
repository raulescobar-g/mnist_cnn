{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, GaussianBlur, RandomPerspective, RandomRotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to normalize the data and convert to a tensor\n",
    "transform = Compose([ToTensor(),\n",
    "    GaussianBlur(kernel_size=(5,9), sigma=(0.01,0.1)),\n",
    "    RandomPerspective(distortion_scale=0.1,p=0.1), # p is probability of being transformed, and scale is degree of distortion\n",
    "    RandomRotation(degrees=10),\n",
    "    Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "# Download the data\n",
    "dataset = FashionMNIST('MNIST_data/', download = True, train = True, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "def show_example(img, label):\n",
    "    print('Label: {} ({})'.format(dataset.classes[label], label))\n",
    "    plt.imshow(img.squeeze(), cmap='Greys_r')\n",
    "    plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_example(*dataset[random.randint(0,len(dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_example(*dataset[20000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_indices(n, val_frac, seed):\n",
    "    # Determine the size of the validation set\n",
    "    n_val = int(val_frac * n)\n",
    "    np.random.seed(seed)\n",
    "    # Create random permutation between 0 to n-1\n",
    "    idxs = np.random.permutation(n)\n",
    "    # Pick first n_val indices for validation set\n",
    "    return idxs[n_val:], idxs[:n_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_frac = 1/10\n",
    "rand_seed =  314159\n",
    "\n",
    "train_indices, val_indices = split_indices(len(dataset), val_frac, rand_seed)\n",
    "print(\"#samples in training set: {}\".format(len(train_indices)))\n",
    "print(\"#samples in validation set: {}\".format(len(val_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training sampler and data loader\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "train_dl = DataLoader(dataset,\n",
    "                     batch_size,\n",
    "                     sampler=train_sampler)\n",
    "\n",
    "# Validation sampler and data loader\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "val_dl = DataLoader(dataset,\n",
    "                   batch_size,\n",
    "                   sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dl):\n",
    "    for images, labels in dl:\n",
    "        fig, ax = plt.subplots(figsize=(10,10))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images, 8).permute(1, 2, 0), cmap='Greys_r')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifierNet(nn.Module):\n",
    "    def __init__(self, n_channels=3):\n",
    "        super(ImageClassifierNet, self).__init__()\n",
    "        ######################\n",
    "        #   YOUR CODE HERE   #\n",
    "        ######################\n",
    "        self.layer1 = nn.Sequential(\n",
    "        nn.Conv2d(1,32,kernel_size=5,stride=1,padding=2),\n",
    "        nn.Sigmoid(),\n",
    "        nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "        nn.Conv2d(32,64,kernel_size=5,stride=1,padding=2),\n",
    "        nn.Sigmoid(),\n",
    "        nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "\n",
    "        self.drop_out = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(7*7*64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageClassifierNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: The total number of parameters should be <= 100,000 for the contest.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, input_size=(batch_size, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable training on a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Use GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_default_device()\n",
    "\n",
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "val_dl = DeviceDataLoader(val_dl, device)\n",
    "\n",
    "to_device(model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(n_epochs, model, train_dl, val_dl, loss_fn, opt_fn, lr):\n",
    "    \"\"\"\n",
    "    Trains the model on a dataset.\n",
    "    \n",
    "    Args:\n",
    "        n_epochs: number of epochs\n",
    "        model: ImageClassifierNet object\n",
    "        train_dl: training dataloader\n",
    "        val_dl: validation dataloader\n",
    "        loss_fn: the loss function\n",
    "        opt_fn: the optimizer\n",
    "        lr: learning rate\n",
    "    \n",
    "    Returns:\n",
    "        The trained model. \n",
    "        A tuple of (model, train_losses, val_losses, train_accuracies, val_accuracies)\n",
    "    \"\"\"\n",
    "    # Record these values the end of each epoch\n",
    "    train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        sum_loss = 0.0\n",
    "        val_l = 0.0\n",
    "        train_ac = 0\n",
    "        val_ac = 0.0\n",
    "        total = 0\n",
    "        vtotal = 0\n",
    "\n",
    "        if len(val_dl) != 0:\n",
    "\n",
    "            for data,vdata in zip(train_dl,val_dl):\n",
    "                model.train()\n",
    "                inputs, labels = data \n",
    "                opt_fn.zero_grad()\n",
    "                outputs = model(inputs)             # output of model\n",
    "                loss = loss_fn(outputs, labels)     # loss of output\n",
    "                loss.backward()                     # learn\n",
    "                opt_fn.step()                       # step in learning direction\n",
    "\n",
    "                sum_loss += loss.item()\n",
    "\n",
    "                _,predicted = torch.max(outputs.data, 1)\n",
    "                train_ac += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    val_inputs, val_labels = vdata\n",
    "                    val_output= model(val_inputs)\n",
    "                    val_loss = loss_fn(val_output, val_labels)\n",
    "\n",
    "                    val_l += val_loss.item()\n",
    "\n",
    "                    _, predicted = torch.max(val_output.data, 1)\n",
    "                    val_ac += (predicted == val_labels).sum().item()\n",
    "                    vtotal += val_labels.size(0)\n",
    "\n",
    "\n",
    "                    \n",
    "            train_losses.append(sum_loss / total)\n",
    "            train_accuracies.append(train_ac / total)\n",
    "            val_losses.append(val_l / vtotal)\n",
    "            val_accuracies.append(val_ac / vtotal)\n",
    "            \n",
    "            print('[epoch: %d] (train loss: %.3f train accuracy: %.3f) (val loss: %.3f val accuracy: %.3f)'% (epoch + 1, sum_loss /total, train_ac/total,val_l/vtotal,val_ac/vtotal))\n",
    "            sum_loss = 0.0\n",
    "            val_l = 0.0\n",
    "            train_ac = 0.0\n",
    "            val_ac = 0.0\n",
    "            total = 0\n",
    "            vtotal = 0\n",
    "\n",
    "        else:\n",
    "            for data in train_dl:\n",
    "                model.train()\n",
    "                inputs, labels = data \n",
    "                opt_fn.zero_grad()\n",
    "                outputs = model(inputs)             # output of model\n",
    "                loss = loss_fn(outputs, labels)     # loss of output\n",
    "                loss.backward()                     # learn\n",
    "                opt_fn.step()                       # step in learning direction\n",
    "\n",
    "                sum_loss += loss.item()\n",
    "\n",
    "                _,predicted = torch.max(outputs.data, 1)\n",
    "                train_ac += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                    \n",
    "                \n",
    "            train_losses.append(sum_loss / total)\n",
    "            train_accuracies.append(train_ac / total)\n",
    "            \n",
    "            print('[epoch: %d] (train loss: %.3f train accuracy: %.3f)'% (epoch + 1, sum_loss / total, train_ac/total))\n",
    "            sum_loss = 0.0\n",
    "            train_ac = 0.0\n",
    "            total = 0\n",
    "\n",
    "\n",
    "    return model, train_losses, val_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "num_epochs = 10\n",
    "loss_fn =  nn.CrossEntropyLoss()\n",
    "lr =  0.01\n",
    "opt_fn =  optim.Adam(model.parameters(),lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_model(num_epochs, model, train_dl, val_dl, loss_fn, opt_fn, lr)\n",
    "model, train_losses, val_losses, train_accuracies, val_accuracies = history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(train_accuracies, val_accuracies):\n",
    "    \"\"\"Plot accuracies\"\"\"\n",
    "    plt.plot(train_accuracies, \"-x\")\n",
    "    plt.plot(val_accuracies, \"-o\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend([\"Training\", \"Validation\"])\n",
    "    plt.title(\"Accuracy vs. No. of epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(train_accuracies, val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(train_losses, val_losses):\n",
    "    \"\"\"Plot losses\"\"\"\n",
    "    plt.plot(train_losses, \"-x\")\n",
    "    plt.plot(val_losses, \"-o\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend([\"Training\", \"Validation\"])\n",
    "    plt.title(\"Loss vs. No. of Epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model on the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices, _ = split_indices(len(dataset), 0, rand_seed)\n",
    "\n",
    "sampler = SubsetRandomSampler(indices)\n",
    "dl = DataLoader(dataset, batch_size, sampler=sampler)\n",
    "dl = DeviceDataLoader(dl, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set the maximum number of training epochs and the learning rate for finetuning your model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 30\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_model(num_epochs, model, dl, [], loss_fn, opt_fn, lr)\n",
    "model = history[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(history[1],history[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_prediction(img, label, probs, classes):\n",
    "    \"\"\"\n",
    "    Visualize predictions.\n",
    "    \"\"\"\n",
    "    probs = probs.cpu().numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(8,15), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).cpu().numpy().squeeze(), cmap='Greys_r')\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('Actual: {}'.format(classes[label]))\n",
    "    ax2.barh(np.arange(10), probs)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_yticklabels(classes, size='small');\n",
    "    ax2.set_title('Predicted: probabilities')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the class probabilites (log softmax) for img\n",
    "images = iter(dl)\n",
    "for imgs, labels in images:\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # Calculate the class probabilites (log softmax) for img\n",
    "        probs = torch.nn.functional.softmax(model(imgs[0].unsqueeze(0)), dim=1)\n",
    "        # Plot the image and probabilites\n",
    "        view_prediction(imgs[0], labels[0], probs, dataset.classes)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very important\n",
    "torch.save(model, 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = FashionMNIST('MNIST_data/', download = True, train = False, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = DataLoader(test_dataset, batch_size)\n",
    "test_dl = DeviceDataLoader(test_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_dl):\n",
    "    \"\"\"\n",
    "    Evaluates your model on the test data.\n",
    "    \n",
    "    Args:\n",
    "        model: ImageClassifierNet object\n",
    "        test_dl: test dataloader\n",
    "    \n",
    "    Returns: \n",
    "        Test accuracy.\n",
    "    \"\"\"\n",
    "   \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_dl:\n",
    "            model.eval()\n",
    "            input, labels = data\n",
    "            output = model(input)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Accuracy = {:.4f}\".format(evaluate(model, test_dl)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
